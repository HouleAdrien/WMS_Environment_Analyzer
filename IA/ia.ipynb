{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_to_label = {\n",
    "    (180, 180, 180): 0,  # Gris - Végétation clairsemée\n",
    "    (0, 100, 0): 1,     # Vert - Couverture arborée\n",
    "    (255, 187, 34): 2,  # Orange - Broussailles\n",
    "    (255, 255, 76): 3,  # Jaune - Prairie\n",
    "    (240, 150, 255): 4, # Rose - Terre cultivée\n",
    "    (250, 0, 0): 5,     # Rouge - Zones urbanisées\n",
    "    (240, 240, 240): 6, # Blanc - Neige et glace\n",
    "    (0, 100, 200): 7,   # Bleu - Plans d'eau permanents\n",
    "    (0, 150, 160): 8,   # Vert-bleu - Marais herbacé\n",
    "    (0, 207, 117): 9,   # Vert mangrove - Mangroves\n",
    "    (250, 230, 160): 10, # Mousse - Lichens\n",
    "    (0, 0, 0): 11       # Noir - Sans données\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_class(label_img):\n",
    "    # Convertir en RGB si nécessaire\n",
    "    label_img = label_img.convert('RGB')\n",
    "    label_array = np.array(label_img)\n",
    "    height, width = label_array.shape[:2]\n",
    "    class_map = np.zeros((height, width), dtype=int)\n",
    "\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            pixel = tuple(label_array[y, x])\n",
    "            class_idx = color_to_label.get(pixel, 11)  # Utilisez -1 ou un autre numéro pour les couleurs non mappées\n",
    "            class_map[y, x] = class_idx\n",
    "\n",
    "    return class_map\n",
    "\n",
    "\n",
    "def load_images_and_masks(satellite_folder, mask_folder):\n",
    "    satellite_images = []\n",
    "    masks = []\n",
    "    for filename in os.listdir(satellite_folder):\n",
    "        sat_img = Image.open(os.path.join(satellite_folder, filename))\n",
    "        mask_img = Image.open(os.path.join(mask_folder, filename))  # Assurez-vous que les noms correspondent\n",
    "\n",
    "        satellite_images.append(np.array(sat_img))\n",
    "        masks.append(label_to_class(mask_img))\n",
    "\n",
    "    return np.array(satellite_images), np.array(masks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_satellite_images = 'TrainData\\Esa_tile2_Sat'  # Chemin vers les images satellites\n",
    "path_labelled_images = 'TrainData\\Esa_tile2_resize'  # Chemin vers les images labelisées (masques)\n",
    "\n",
    "X, Y = load_images_and_masks(path_satellite_images, path_labelled_images)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def EncoderMiniBlock(inputs, n_filters=12, dropout_prob=0.3, max_pooling=True):\n",
    "    conv = Conv2D(n_filters, \n",
    "                  3,  # filter size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='HeNormal')(inputs)\n",
    "    conv = Conv2D(n_filters, \n",
    "                  3,  # filter size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='HeNormal')(conv)\n",
    "  \n",
    "    conv = BatchNormalization()(conv, training=False)\n",
    "    if dropout_prob > 0:     \n",
    "        conv = tf.keras.layers.Dropout(dropout_prob)(conv)\n",
    "    if max_pooling:\n",
    "        next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2,2))(conv)    \n",
    "    else:\n",
    "        next_layer = conv\n",
    "    skip_connection = conv    \n",
    "    return next_layer, skip_connection\n",
    "\n",
    "def DecoderMiniBlock(prev_layer_input, skip_layer_input, n_filters=12):\n",
    "    up = Conv2DTranspose(\n",
    "                 n_filters,\n",
    "                 (3,3),\n",
    "                 strides=(2,2),\n",
    "                 padding='same')(prev_layer_input)\n",
    "    merge = concatenate([up, skip_layer_input], axis=3)\n",
    "    conv = Conv2D(n_filters, \n",
    "                 3,  \n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='HeNormal')(merge)\n",
    "    conv = Conv2D(n_filters,\n",
    "                 3, \n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='HeNormal')(conv)\n",
    "    return conv\n",
    "\n",
    "def create_unet_model(input_shape, num_classes,n_filters=32):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "  # Encoder includes multiple convolutional mini blocks with different maxpooling, dropout and filter parameters\n",
    "    # Observe that the filters are increasing as we go deeper into the network which will increasse the # channels of the image \n",
    "    cblock1 = EncoderMiniBlock(inputs, n_filters,dropout_prob=0, max_pooling=True)\n",
    "    cblock2 = EncoderMiniBlock(cblock1[0],n_filters*2,dropout_prob=0, max_pooling=True)\n",
    "    cblock3 = EncoderMiniBlock(cblock2[0], n_filters*4,dropout_prob=0, max_pooling=True)\n",
    "    cblock4 = EncoderMiniBlock(cblock3[0], n_filters*8,dropout_prob=0.3, max_pooling=True)\n",
    "    cblock5 = EncoderMiniBlock(cblock4[0], n_filters*16, dropout_prob=0.3, max_pooling=False) \n",
    "    \n",
    "    # Decoder includes multiple mini blocks with decreasing number of filters\n",
    "    # Observe the skip connections from the encoder are given as input to the decoder\n",
    "    # Recall the 2nd output of encoder block was skip connection, hence cblockn[1] is used\n",
    "    ublock6 = DecoderMiniBlock(cblock5[0], cblock4[1],  n_filters * 8)\n",
    "    ublock7 = DecoderMiniBlock(ublock6, cblock3[1],  n_filters * 4)\n",
    "    ublock8 = DecoderMiniBlock(ublock7, cblock2[1],  n_filters * 2)\n",
    "    ublock9 = DecoderMiniBlock(ublock8, cblock1[1],  n_filters)\n",
    "\n",
    "    conv9 = Conv2D(n_filters,\n",
    "                 3,\n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='he_normal')(ublock9)\n",
    "\n",
    "    conv10 = Conv2D(num_classes, 1, padding='same')(conv9)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=conv10)\n",
    "    return model\n",
    "\n",
    "# Remplacer par la forme réelle de vos images et le nombre de canaux (par exemple, (1440, 1440, 3) pour des images RGB)\n",
    "model = create_unet_model((1440, 1440, 3), num_classes=12)\n",
    "model.compile(optimizer='adam', \n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),  # Assuming softmax activation in the last layer\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "30/30 [==============================] - 10473s 350s/step - loss: 14.6337 - accuracy: 0.3171 - val_loss: 2.2316 - val_accuracy: 0.5020\n",
      "Epoch 2/7\n",
      "30/30 [==============================] - 10289s 344s/step - loss: 1.9662 - accuracy: 0.5055 - val_loss: 1.6904 - val_accuracy: 0.5477\n",
      "Epoch 3/7\n",
      "30/30 [==============================] - 10794s 362s/step - loss: 1.4751 - accuracy: 0.5280 - val_loss: 1.3193 - val_accuracy: 0.5891\n",
      "Epoch 4/7\n",
      "30/30 [==============================] - 10405s 348s/step - loss: 1.2046 - accuracy: 0.5899 - val_loss: 1.1842 - val_accuracy: 0.6103\n",
      "Epoch 5/7\n",
      "30/30 [==============================] - 11678s 390s/step - loss: 1.1388 - accuracy: 0.6175 - val_loss: 1.1690 - val_accuracy: 0.6388\n",
      "Epoch 6/7\n",
      "30/30 [==============================] - 11451s 384s/step - loss: 1.0495 - accuracy: 0.6504 - val_loss: 1.1238 - val_accuracy: 0.6387\n",
      "Epoch 7/7\n",
      "30/30 [==============================] - 11449s 383s/step - loss: 1.0225 - accuracy: 0.6514 - val_loss: 1.1483 - val_accuracy: 0.6251\n",
      "2/2 [==============================] - 744s 301s/step - loss: 1.1483 - accuracy: 0.6251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1483187675476074, 0.6251271367073059]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=8, epochs=7, validation_data=(X_test, Y_test))\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 8s 8s/step\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.metrics import SparseCategoricalCrossentropy\n",
    "\n",
    "label_to_color = {v: k for k, v in color_to_label.items()}\n",
    "\n",
    "# Charger et prétraiter l'image\n",
    "def load_and_preprocess_image(image_path, target_size):\n",
    "    image = load_img(image_path, target_size=target_size) # Assurez-vous que target_size correspond à la taille d'entrée du modèle\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0) # Ajouter une dimension pour le batch\n",
    "    return image\n",
    "\n",
    "def prediction_to_image(prediction):\n",
    "    # Prendre l'indice de la classe la plus probable\n",
    "    prediction = np.argmax(prediction, axis=-1)\n",
    "\n",
    "    # Initialiser une image RGB vide\n",
    "    colored_prediction = np.zeros((prediction.shape[0], prediction.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    # Appliquer la couleur à chaque pixel\n",
    "    for label, color in label_to_color.items():\n",
    "        colored_prediction[prediction == label] = color\n",
    "\n",
    "    return Image.fromarray(colored_prediction)\n",
    "\n",
    "# Faire une prédiction et afficher avec Pillow\n",
    "def predict_and_display(model, image_path, target_size=(1440, 1440)):\n",
    "    image = load_and_preprocess_image(image_path, target_size)\n",
    "    prediction = model.predict(image)[0] # Récupérer la prédiction pour le premier élément du batch\n",
    "\n",
    "    # Convertir la prédiction en image PIL et afficher\n",
    "    original_image = Image.open(image_path)\n",
    "    predicted_image = prediction_to_image(prediction)\n",
    "\n",
    "    original_image.show(title='Image Originale')\n",
    "    predicted_image.show(title='Prédiction du Modèle')\n",
    "\n",
    "# Exemple d'utilisation\n",
    "image_path = 'TrainData/Esa_tile2_Sat/ESA_WorldCover_10m_2021_V200_N36E027_Map.tif' # Remplacer par le chemin de votre image\n",
    "predict_and_display(model, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"testmodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "30/30 [==============================] - 11378s 377s/step - loss: 1.0422 - accuracy: 0.6500 - val_loss: 1.0522 - val_accuracy: 0.6441\n",
      "Epoch 2/3\n",
      "30/30 [==============================] - 11453s 380s/step - loss: 1.0609 - accuracy: 0.6491 - val_loss: 1.1029 - val_accuracy: 0.6257\n",
      "Epoch 3/3\n",
      "30/30 [==============================] - 11049s 368s/step - loss: 0.9904 - accuracy: 0.6606 - val_loss: 1.0449 - val_accuracy: 0.6482\n",
      "2/2 [==============================] - 744s 295s/step - loss: 1.0449 - accuracy: 0.6482\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('testmodel.h5')\n",
    "model.fit(X_train, Y_train, batch_size=8, epochs=3, validation_data=(X_test, Y_test))\n",
    "model.evaluate(X_test, Y_test)\n",
    "model.save(\"testmodelv2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n"
     ]
    }
   ],
   "source": [
    "image_path = 'TrainData/Esa_tile2_Sat/ESA_WorldCover_10m_2021_V200_N36E027_Map.tif' # Remplacer par le chemin de votre image\n",
    "predict_and_display(model, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
