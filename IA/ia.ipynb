{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\Yahnis\\.conda\\envs\\env_ia\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Une routine d’initialisation d’une bibliothèque de liens dynamiques (DLL) a échoué.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\env_ia\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:62\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: Une routine d’initialisation d’une bibliothèque de liens dynamiques (DLL) a échoué.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_ia\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_ia\\lib\\site-packages\\tensorflow\\python\\__init__.py:36\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_ia\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:77\u001b[0m\n\u001b[0;32m     75\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     78\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     79\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     80\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     81\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     82\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     83\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     84\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\Yahnis\\.conda\\envs\\env_ia\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Une routine d’initialisation d’une bibliothèque de liens dynamiques (DLL) a échoué.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_to_labels(mask_img, color_to_label):\n",
    "    label_mask = np.zeros((mask_img.shape[0], mask_img.shape[1]), dtype=np.int32)\n",
    "    for rgb, label in color_to_label.items():\n",
    "        mask = np.all(mask_img == np.array(rgb, dtype=np.uint8), axis=-1)\n",
    "        label_mask[mask] = label\n",
    "    return label_mask\n",
    "\n",
    "def load_images_and_masks(satellite_folder, mask_folder):\n",
    "    satellite_images = []\n",
    "    masks = []\n",
    "    for filename in os.listdir(satellite_folder):\n",
    "        sat_img = Image.open(os.path.join(satellite_folder, filename))\n",
    "        mask_img = Image.open(os.path.join(mask_folder, filename))  # Assurez-vous que les noms correspondent\n",
    "\n",
    "        satellite_images.append(np.array(sat_img))\n",
    "        converted_mask = convert_rgb_to_labels(np.array(mask_img), color_to_label)\n",
    "        masks.append(converted_mask)\n",
    "\n",
    "    return np.array(satellite_images), np.array(masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_to_label = {\n",
    "    (180, 180, 180): 0,  # Gris - Végétation clairsemée\n",
    "    (0, 100, 0): 1,     # Vert - Couverture arborée\n",
    "    (255, 187, 34): 2,  # Orange - Broussailles\n",
    "    (255, 255, 76): 3,  # Jaune - Prairie\n",
    "    (240, 150, 255): 4, # Rose - Terre cultivée\n",
    "    (250, 0, 0): 5,     # Rouge - Zones urbanisées\n",
    "    (240, 240, 240): 6, # Blanc - Neige et glace\n",
    "    (0, 100, 200): 7,   # Bleu - Plans d'eau permanents\n",
    "    (0, 150, 160): 8,   # Vert-bleu - Marais herbacé\n",
    "    (0, 207, 117): 9,   # Vert mangrove - Mangroves\n",
    "    (250, 230, 160): 10 # Mousse - Lichens\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adrie\\AppData\\Local\\Temp\\ipykernel_12544\\3415542517.py:4: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  mask = np.all(mask_img == np.array(rgb, dtype=np.uint8), axis=-1)\n"
     ]
    }
   ],
   "source": [
    "path_satellite_images = 'TrainData\\Esa_tile1_Sat'  # Chemin vers les images satellites\n",
    "path_labelled_images = 'TrainData\\Esa_tile1_resize'  # Chemin vers les images labelisées (masques)\n",
    "\n",
    "X, Y = load_images_and_masks(path_satellite_images, path_labelled_images)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unet_model(input_shape, num_classes):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Couches descendantes\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    \n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "    \n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "\n",
    "    # Couches ascendantes\n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    \n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    \n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    \n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(c9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Remplacer par la forme réelle de vos images et le nombre de canaux (par exemple, (1440, 1440, 3) pour des images RGB)\n",
    "model = create_unet_model((1440, 1440, 3), num_classes=11)\n",
    "model.compile(optimizer='adam', \n",
    "              loss=SparseCategoricalCrossentropy(from_logits=False),  # Assuming softmax activation in the last layer\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 601s 18s/step - loss: 2.4772 - accuracy: 0.9454 - val_loss: 8.8523e-06 - val_accuracy: 1.0000\n",
      "2/2 [==============================] - 38s 2s/step - loss: 8.8523e-06 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.852269274939317e-06, 0.9999993443489075]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=4, epochs=1, validation_data=(X_test, Y_test))\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 733ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 1440), |u1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\env_ia\\lib\\site-packages\\PIL\\Image.py:3070\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3069\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3070\u001b[0m     mode, rawmode \u001b[38;5;241m=\u001b[39m \u001b[43m_fromarray_typemap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypekey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   3071\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: ((1, 1, 1440), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Exemple d'utilisation\u001b[39;00m\n\u001b[0;32m     32\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrainData/Esa_tile1_Sat/ESA_WorldCover_10m_2021_V200_N33W006_Map.tif\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Remplacer par le chemin de votre image\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mpredict_and_display\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m, in \u001b[0;36mpredict_and_display\u001b[1;34m(model, image_path, target_size)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Convertir la prédiction en image PIL et afficher\u001b[39;00m\n\u001b[0;32m     25\u001b[0m original_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n\u001b[1;32m---> 26\u001b[0m predicted_image \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m original_image\u001b[38;5;241m.\u001b[39mshow(title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage Originale\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m predicted_image\u001b[38;5;241m.\u001b[39mshow(title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrédiction du Modèle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m, in \u001b[0;36mprediction_to_image\u001b[1;34m(prediction)\u001b[0m\n\u001b[0;32m     15\u001b[0m prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(prediction, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Prendre l'indice de la classe la plus probable\u001b[39;00m\n\u001b[0;32m     16\u001b[0m prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8(prediction \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m/\u001b[39m prediction\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;66;03m# Convertir en valeurs d'octets pour l'affichage\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_ia\\lib\\site-packages\\PIL\\Image.py:3073\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3071\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   3072\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m typekey\n\u001b[1;32m-> 3073\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   3074\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3075\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 1440), |u1"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.metrics import SparseCategoricalCrossentropy\n",
    "\n",
    "# Charger et prétraiter l'image\n",
    "def load_and_preprocess_image(image_path, target_size):\n",
    "    image = load_img(image_path, target_size=target_size) # Assurez-vous que target_size correspond à la taille d'entrée du modèle\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0) # Ajouter une dimension pour le batch\n",
    "    return image\n",
    "\n",
    "# Convertir une prédiction en image PIL\n",
    "def prediction_to_image(prediction):\n",
    "    prediction = np.argmax(prediction, axis=-1) # Prendre l'indice de la classe la plus probable\n",
    "    prediction = np.uint8(prediction * 255 / prediction.max()) # Convertir en valeurs d'octets pour l'affichage\n",
    "    return Image.fromarray(prediction)\n",
    "\n",
    "# Faire une prédiction et afficher avec Pillow\n",
    "def predict_and_display(model, image_path, target_size=(1440, 1440)):\n",
    "    image = load_and_preprocess_image(image_path, target_size)\n",
    "    prediction = model.predict(image)[0] # Récupérer la prédiction pour le premier élément du batch\n",
    "\n",
    "    # Convertir la prédiction en image PIL et afficher\n",
    "    original_image = Image.open(image_path)\n",
    "    predicted_image = prediction_to_image(prediction)\n",
    "\n",
    "    original_image.show(title='Image Originale')\n",
    "    predicted_image.show(title='Prédiction du Modèle')\n",
    "\n",
    "# Exemple d'utilisation\n",
    "image_path = 'TrainData/Esa_tile1_Sat/ESA_WorldCover_10m_2021_V200_N33W006_Map.tif' # Remplacer par le chemin de votre image\n",
    "predict_and_display(model, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
