{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\Yahnis\\.conda\\envs\\env_ia\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Une routine d’initialisation d’une bibliothèque de liens dynamiques (DLL) a échoué.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\env_ia\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:62\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: Une routine d’initialisation d’une bibliothèque de liens dynamiques (DLL) a échoué.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_ia\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_ia\\lib\\site-packages\\tensorflow\\python\\__init__.py:36\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\env_ia\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:77\u001b[0m\n\u001b[0;32m     75\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     78\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     79\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     80\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     81\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     82\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     83\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     84\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\Yahnis\\.conda\\envs\\env_ia\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Une routine d’initialisation d’une bibliothèque de liens dynamiques (DLL) a échoué.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_to_label = {\n",
    "    (180, 180, 180): 0,  # Gris - Végétation clairsemée\n",
    "    (0, 100, 0): 1,     # Vert - Couverture arborée\n",
    "    (255, 187, 34): 2,  # Orange - Broussailles\n",
    "    (255, 255, 76): 3,  # Jaune - Prairie\n",
    "    (240, 150, 255): 4, # Rose - Terre cultivée\n",
    "    (250, 0, 0): 5,     # Rouge - Zones urbanisées\n",
    "    (240, 240, 240): 6, # Blanc - Neige et glace\n",
    "    (0, 100, 200): 7,   # Bleu - Plans d'eau permanents\n",
    "    (0, 150, 160): 8,   # Vert-bleu - Marais herbacé\n",
    "    (0, 207, 117): 9,   # Vert mangrove - Mangroves\n",
    "    (250, 230, 160): 10, # Mousse - Lichens\n",
    "    (0, 0, 0): 11       # Noir - Sans données\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_class(label_img):\n",
    "    # Convertir en RGB si nécessaire\n",
    "    label_img = label_img.convert('RGB')\n",
    "    label_array = np.array(label_img)\n",
    "    height, width = label_array.shape[:2]\n",
    "    class_map = np.zeros((height, width), dtype=int)\n",
    "\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            pixel = tuple(label_array[y, x])\n",
    "            class_idx = color_to_label.get(pixel, 11)  # Utilisez -1 ou un autre numéro pour les couleurs non mappées\n",
    "            class_map[y, x] = class_idx\n",
    "\n",
    "    return class_map\n",
    "\n",
    "\n",
    "def load_images_and_masks(satellite_folder, mask_folder):\n",
    "    satellite_images = []\n",
    "    masks = []\n",
    "    for filename in os.listdir(satellite_folder):\n",
    "        sat_img = Image.open(os.path.join(satellite_folder, filename))\n",
    "        mask_img = Image.open(os.path.join(mask_folder, filename))  # Assurez-vous que les noms correspondent\n",
    "\n",
    "        satellite_images.append(np.array(sat_img))\n",
    "        masks.append(label_to_class(mask_img))\n",
    "\n",
    "    return np.array(satellite_images), np.array(masks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_satellite_images = 'TrainData\\Esa_tile1_Sat'  # Chemin vers les images satellites\n",
    "path_labelled_images = 'TrainData\\Esa_tile1_resize'  # Chemin vers les images labelisées (masques)\n",
    "\n",
    "X, Y = load_images_and_masks(path_satellite_images, path_labelled_images)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def EncoderMiniBlock(inputs, n_filters=12, dropout_prob=0.3, max_pooling=True):\n",
    "    conv = Conv2D(n_filters, \n",
    "                  3,  # filter size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='HeNormal')(inputs)\n",
    "    conv = Conv2D(n_filters, \n",
    "                  3,  # filter size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='HeNormal')(conv)\n",
    "  \n",
    "    conv = BatchNormalization()(conv, training=False)\n",
    "    if dropout_prob > 0:     \n",
    "        conv = tf.keras.layers.Dropout(dropout_prob)(conv)\n",
    "    if max_pooling:\n",
    "        next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2,2))(conv)    \n",
    "    else:\n",
    "        next_layer = conv\n",
    "    skip_connection = conv    \n",
    "    return next_layer, skip_connection\n",
    "\n",
    "def DecoderMiniBlock(prev_layer_input, skip_layer_input, n_filters=12):\n",
    "    up = Conv2DTranspose(\n",
    "                 n_filters,\n",
    "                 (3,3),\n",
    "                 strides=(2,2),\n",
    "                 padding='same')(prev_layer_input)\n",
    "    merge = concatenate([up, skip_layer_input], axis=3)\n",
    "    conv = Conv2D(n_filters, \n",
    "                 3,  \n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='HeNormal')(merge)\n",
    "    conv = Conv2D(n_filters,\n",
    "                 3, \n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='HeNormal')(conv)\n",
    "    return conv\n",
    "\n",
    "def create_unet_model(input_shape, num_classes,n_filters=32):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "  # Encoder includes multiple convolutional mini blocks with different maxpooling, dropout and filter parameters\n",
    "    # Observe that the filters are increasing as we go deeper into the network which will increasse the # channels of the image \n",
    "    cblock1 = EncoderMiniBlock(inputs, n_filters,dropout_prob=0, max_pooling=True)\n",
    "    cblock2 = EncoderMiniBlock(cblock1[0],n_filters*2,dropout_prob=0, max_pooling=True)\n",
    "    cblock3 = EncoderMiniBlock(cblock2[0], n_filters*4,dropout_prob=0, max_pooling=True)\n",
    "    cblock4 = EncoderMiniBlock(cblock3[0], n_filters*8,dropout_prob=0.3, max_pooling=True)\n",
    "    cblock5 = EncoderMiniBlock(cblock4[0], n_filters*16, dropout_prob=0.3, max_pooling=False) \n",
    "    \n",
    "    # Decoder includes multiple mini blocks with decreasing number of filters\n",
    "    # Observe the skip connections from the encoder are given as input to the decoder\n",
    "    # Recall the 2nd output of encoder block was skip connection, hence cblockn[1] is used\n",
    "    ublock6 = DecoderMiniBlock(cblock5[0], cblock4[1],  n_filters * 8)\n",
    "    ublock7 = DecoderMiniBlock(ublock6, cblock3[1],  n_filters * 4)\n",
    "    ublock8 = DecoderMiniBlock(ublock7, cblock2[1],  n_filters * 2)\n",
    "    ublock9 = DecoderMiniBlock(ublock8, cblock1[1],  n_filters)\n",
    "\n",
    "    conv9 = Conv2D(n_filters,\n",
    "                 3,\n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='he_normal')(ublock9)\n",
    "\n",
    "    conv10 = Conv2D(num_classes, 1, padding='same')(conv9)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=conv10)\n",
    "    return model\n",
    "\n",
    "# Remplacer par la forme réelle de vos images et le nombre de canaux (par exemple, (1440, 1440, 3) pour des images RGB)\n",
    "model = create_unet_model((1440, 1440, 3), num_classes=12)\n",
    "model.compile(optimizer='adam', \n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),  # Assuming softmax activation in the last layer\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1/17 [>.............................] - ETA: 2:00:45 - loss: 83.8133 - accuracy: 0.0373"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=8, epochs=10, validation_data=(X_test, Y_test))\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.metrics import SparseCategoricalCrossentropy\n",
    "\n",
    "label_to_color = {v: k for k, v in color_to_label.items()}\n",
    "\n",
    "# Charger et prétraiter l'image\n",
    "def load_and_preprocess_image(image_path, target_size):\n",
    "    image = load_img(image_path, target_size=target_size) # Assurez-vous que target_size correspond à la taille d'entrée du modèle\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0) # Ajouter une dimension pour le batch\n",
    "    return image\n",
    "\n",
    "def prediction_to_image(prediction):\n",
    "    # Prendre l'indice de la classe la plus probable\n",
    "    prediction = np.argmax(prediction, axis=-1)\n",
    "\n",
    "    # Initialiser une image RGB vide\n",
    "    colored_prediction = np.zeros((prediction.shape[0], prediction.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    # Appliquer la couleur à chaque pixel\n",
    "    for label, color in label_to_color.items():\n",
    "        colored_prediction[prediction == label] = color\n",
    "\n",
    "    return Image.fromarray(colored_prediction)\n",
    "\n",
    "# Faire une prédiction et afficher avec Pillow\n",
    "def predict_and_display(model, image_path, target_size=(1440, 1440)):\n",
    "    image = load_and_preprocess_image(image_path, target_size)\n",
    "    prediction = model.predict(image)[0] # Récupérer la prédiction pour le premier élément du batch\n",
    "\n",
    "    # Convertir la prédiction en image PIL et afficher\n",
    "    original_image = Image.open(image_path)\n",
    "    predicted_image = prediction_to_image(prediction)\n",
    "\n",
    "    original_image.show(title='Image Originale')\n",
    "    predicted_image.show(title='Prédiction du Modèle')\n",
    "\n",
    "# Exemple d'utilisation\n",
    "image_path = 'TrainData/Esa_tile1_Sat/ESA_WorldCover_10m_2021_V200_N33W006_Map.tif' # Remplacer par le chemin de votre image\n",
    "predict_and_display(model, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"testmodel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
