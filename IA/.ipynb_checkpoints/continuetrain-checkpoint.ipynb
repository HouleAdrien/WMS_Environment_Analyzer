{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Conv2DTranspose,\n",
    "    concatenate,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, binary_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.metrics import SparseCategoricalCrossentropy\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tensorflow.keras.layers import Input  \n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_to_label = {\n",
    "    (180, 180, 180): 0,  # Gris - Végétation clairsemée\n",
    "    (0, 100, 0): 1,     # Vert - Couverture arborée\n",
    "    (255, 187, 34): 2,  # Orange - Broussailles\n",
    "    (255, 255, 76): 3,  # Jaune - Prairie\n",
    "    (240, 150, 255): 4, # Rose - Terre cultivée\n",
    "    (250, 0, 0): 5,     # Rouge - Zones urbanisées\n",
    "    (240, 240, 240): 6, # Blanc - Neige et glace\n",
    "    (0, 100, 200): 7,   # Bleu - Plans d'eau permanents\n",
    "    (0, 150, 160): 8,   # Vert-bleu - Marais herbacé\n",
    "    (0, 207, 117): 9,   # Vert mangrove - Mangroves\n",
    "    (250, 230, 160): 10, # Mousse - Lichens\n",
    "    (0, 0, 0): 11       # Noir - Sans données\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_class(label_img):\n",
    "    # Convertir en RGB si nécessaire\n",
    "    label_img = label_img.convert('RGB')\n",
    "    label_array = np.array(label_img)\n",
    "    height, width = label_array.shape[:2]\n",
    "    class_map = np.zeros((height, width), dtype=int)\n",
    "\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            pixel = tuple(label_array[y, x])\n",
    "            class_idx = color_to_label.get(pixel, 11)  # Utilisez -1 ou un autre numéro pour les couleurs non mappées\n",
    "            class_map[y, x] = class_idx\n",
    "\n",
    "    return class_map\n",
    "\n",
    "\n",
    "def load_images_and_masks(satellite_folder, mask_folder):\n",
    "    satellite_images = []\n",
    "    masks = []\n",
    "    for filename in os.listdir(satellite_folder):\n",
    "        sat_img = Image.open(os.path.join(satellite_folder, filename))\n",
    "        mask_img = Image.open(os.path.join(mask_folder, filename))  # Assurez-vous que les noms correspondent\n",
    "\n",
    "        satellite_images.append(np.array(sat_img))\n",
    "        masks.append(label_to_class(mask_img))\n",
    "\n",
    "    return np.array(satellite_images), np.array(masks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_satellite_images = 'TrainData\\Esa_tile3_Sat'  # Chemin vers les images satellites\n",
    "path_labelled_images = 'TrainData\\Esa_tile3_resize'  # Chemin vers les images labelisées (masques)\n",
    "\n",
    "X, Y = load_images_and_masks(path_satellite_images, path_labelled_images)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def EncoderMiniBlock(inputs, n_filters=12, dropout_prob=0.3, max_pooling=True):\n",
    "    conv = Conv2D(n_filters, \n",
    "                  3,  # filter size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='HeNormal')(inputs)\n",
    "    conv = Conv2D(n_filters, \n",
    "                  3,  # filter size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='HeNormal')(conv)\n",
    "  \n",
    "    conv = BatchNormalization()(conv, training=False)\n",
    "    if dropout_prob > 0:     \n",
    "        conv = tf.keras.layers.Dropout(dropout_prob)(conv)\n",
    "    if max_pooling:\n",
    "        next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2,2))(conv)    \n",
    "    else:\n",
    "        next_layer = conv\n",
    "    skip_connection = conv    \n",
    "    return next_layer, skip_connection\n",
    "\n",
    "def DecoderMiniBlock(prev_layer_input, skip_layer_input, n_filters=12):\n",
    "    up = Conv2DTranspose(\n",
    "                 n_filters,\n",
    "                 (3,3),\n",
    "                 strides=(2,2),\n",
    "                 padding='same')(prev_layer_input)\n",
    "    merge = concatenate([up, skip_layer_input], axis=3)\n",
    "    conv = Conv2D(n_filters, \n",
    "                 3,  \n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='HeNormal')(merge)\n",
    "    conv = Conv2D(n_filters,\n",
    "                 3, \n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='HeNormal')(conv)\n",
    "    return conv\n",
    "\n",
    "def create_unet_model(input_shape, num_classes,n_filters=32):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "  # Encoder includes multiple convolutional mini blocks with different maxpooling, dropout and filter parameters\n",
    "    # Observe that the filters are increasing as we go deeper into the network which will increasse the # channels of the image \n",
    "    cblock1 = EncoderMiniBlock(inputs, n_filters,dropout_prob=0, max_pooling=True)\n",
    "    cblock2 = EncoderMiniBlock(cblock1[0],n_filters*2,dropout_prob=0, max_pooling=True)\n",
    "    cblock3 = EncoderMiniBlock(cblock2[0], n_filters*4,dropout_prob=0, max_pooling=True)\n",
    "    cblock4 = EncoderMiniBlock(cblock3[0], n_filters*8,dropout_prob=0.3, max_pooling=True)\n",
    "    cblock5 = EncoderMiniBlock(cblock4[0], n_filters*16, dropout_prob=0.3, max_pooling=False) \n",
    "    \n",
    "    # Decoder includes multiple mini blocks with decreasing number of filters\n",
    "    # Observe the skip connections from the encoder are given as input to the decoder\n",
    "    # Recall the 2nd output of encoder block was skip connection, hence cblockn[1] is used\n",
    "    ublock6 = DecoderMiniBlock(cblock5[0], cblock4[1],  n_filters * 8)\n",
    "    ublock7 = DecoderMiniBlock(ublock6, cblock3[1],  n_filters * 4)\n",
    "    ublock8 = DecoderMiniBlock(ublock7, cblock2[1],  n_filters * 2)\n",
    "    ublock9 = DecoderMiniBlock(ublock8, cblock1[1],  n_filters)\n",
    "\n",
    "    conv9 = Conv2D(n_filters,\n",
    "                 3,\n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='he_normal')(ublock9)\n",
    "\n",
    "    conv10 = Conv2D(num_classes, 1, padding='same')(conv9)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=conv10)\n",
    "    return model\n",
    "\n",
    "# Remplacer par la forme réelle de vos images et le nombre de canaux (par exemple, (1440, 1440, 3) pour des images RGB)\n",
    "model = create_unet_model((1440, 1440, 3), num_classes=12)\n",
    "model.compile(optimizer='adam', \n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),  # Assuming softmax activation in the last layer\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "31/31 [==============================] - 10231s 332s/step - loss: 1.0679 - accuracy: 0.6231 - val_loss: 0.9458 - val_accuracy: 0.6610\n",
      "Epoch 2/4\n",
      "31/31 [==============================] - 11922s 387s/step - loss: 0.9668 - accuracy: 0.6404 - val_loss: 0.9351 - val_accuracy: 0.6576\n",
      "Epoch 3/4\n",
      "31/31 [==============================] - 11421s 370s/step - loss: 0.9539 - accuracy: 0.6414 - val_loss: 0.9177 - val_accuracy: 0.6722\n",
      "Epoch 4/4\n",
      "31/31 [==============================] - 11600s 376s/step - loss: 0.8942 - accuracy: 0.6532 - val_loss: 0.8603 - val_accuracy: 0.6878\n",
      "1/2 [==============>...............] - ETA: 7:01 - loss: 1.0026 - accuracy: 0.6392"
     ]
    }
   ],
   "source": [
    "model = load_model('testmodelv3.h5')\n",
    "model.fit(X_train, Y_train, batch_size=8, epochs=4, validation_data=(X_test, Y_test))\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_to_color = {v: k for k, v in color_to_label.items()}\n",
    "\n",
    "# Charger et prétraiter l'image\n",
    "def load_and_preprocess_image(image_path, target_size):\n",
    "    image = load_img(image_path, target_size=target_size) # Assurez-vous que target_size correspond à la taille d'entrée du modèle\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0) # Ajouter une dimension pour le batch\n",
    "    return image\n",
    "\n",
    "def prediction_to_image(prediction):\n",
    "    # Prendre l'indice de la classe la plus probable\n",
    "    prediction = np.argmax(prediction, axis=-1)\n",
    "\n",
    "    # Initialiser une image RGB vide\n",
    "    colored_prediction = np.zeros((prediction.shape[0], prediction.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    # Appliquer la couleur à chaque pixel\n",
    "    for label, color in label_to_color.items():\n",
    "        colored_prediction[prediction == label] = color\n",
    "\n",
    "    return Image.fromarray(colored_prediction)\n",
    "\n",
    "# Faire une prédiction et afficher avec Pillow\n",
    "def predict_and_display(model, image_path, target_size=(1440, 1440)):\n",
    "    image = load_and_preprocess_image(image_path, target_size)\n",
    "    prediction = model.predict(image)[0] # Récupérer la prédiction pour le premier élément du batch\n",
    "\n",
    "    # Convertir la prédiction en image PIL et afficher\n",
    "    original_image = Image.open(image_path)\n",
    "    predicted_image = prediction_to_image(prediction)\n",
    "\n",
    "    original_image.show(title='Image Originale')\n",
    "    predicted_image.show(title='Prédiction du Modèle')\n",
    "\n",
    "# Exemple d'utilisation\n",
    "image_path = 'TrainData/Esa_tile2_Sat/ESA_WorldCover_10m_2021_V200_N36E027_Map.tif' # Remplacer par le chemin de votre image\n",
    "predict_and_display(model, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"testmodelv4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = np.argmax(predictions, axis=3)\n",
    "\n",
    "# Flatten the ground truth and predicted labels for confusion matrix calculation\n",
    "flat_true_labels = Y_test.flatten()\n",
    "flat_pred_labels = predicted_labels.flatten()\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(flat_true_labels, flat_pred_labels, normalize='true')  # normalize='all' for overall normalization\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\".2%\", cmap='Blues', xticklabels=color_to_label.keys(), yticklabels=color_to_label.keys())\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "class_names = [str(label) for label in color_to_label.keys()]\n",
    "print(classification_report(flat_true_labels, flat_pred_labels, target_names=class_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
